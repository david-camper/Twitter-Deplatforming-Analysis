{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545abecb-f6ce-432b-967c-2c717f477859",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"background-color: #431170; padding: 20px;\">\n",
    "<img src=\"https://macss.berkeley.edu/wp-content/uploads/2023/09/UCBMaCSS_Logo_2Color_Reverse_TaglineB.png\" alt=\"MaCSS\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "# **Assignment 3:** Did deplatforming reduce misinformation on Twitter? \n",
    "\n",
    "[wdtmacss@berkeley.edu](mailto:wdtmacss@berkeley.edu)\\\n",
    "**Computational Social Science 1A**\\\n",
    "[Human Psychology and Social Technologies](https://classes.berkeley.edu/content/2024-fall-compss-214a-001-lec-001) \n",
    "Fall 2024\\\n",
    "UC Berkeley [Masters in Computational Social Science](https://macss.berkeley.edu/about/)\n",
    "\n",
    "👩🏾‍🔬🧑‍💻👩🏻‍💻👨🏿‍💻🔬\n",
    "\n",
    "---\n",
    "\n",
    "Your goal is to write a brief computational essay providing data-driven answers to the question posed in the title and reflections on the strengths and limitations of the dataset and the Difference in Differences method. **Detailed guidelines for completing this assignment are avilable [here](https://bcourses.berkeley.edu/files/89852410/download?download_frd=1).**\n",
    "\n",
    "**Practical Instructions:** \n",
    "\n",
    "*  Take a copy of this notebook and complete Sections 2 - 5. Add as many code and markdown cells as you need within those sections.\n",
    "*  Answer the External Resources question in Section 6.\n",
    "*  Submit your completed notebook through gradescope.\n",
    "\n",
    "**Due date:** 10/14/2024 (before midnight Pacific time)\n",
    "\n",
    "**Grading guidelines** are included in the assignment description [here](https://bcourses.berkeley.edu/files/89852410/download?download_frd=1).\n",
    "\n",
    "**The AI model usage policy** is available [here](https://bcourses.berkeley.edu/courses/1538139/files?preview=89785318).\n",
    "\n",
    "**Class materials**\n",
    "You are welcome to make use of the materials we have developed during this class. The original notebooks can be accessed below, and you are welcome to also consult your own copies of the notebooks you worked on during lab sessions.\n",
    "\n",
    "*  Notebook 1: [Data Acquisition](https://github.com/ccs-ucb/CSS1A/blob/main/notebooks/notebook-1-data-acquisition.ipynb)\n",
    "*  Notebook 2: [Data Exploration](https://github.com/ccs-ucb/CSS1A/blob/main/notebooks/notebook-2-data-exploration.ipynb)\n",
    "*  Notebook 3: [Data Simulation](https://github.com/ccs-ucb/CSS1A/blob/main/notebooks/notebook-3-data-analysis.ipynb)\n",
    "*  Notebook 4: [Data Analysis](https://github.com/ccs-ucb/CSS1A/blob/main/notebooks/notebook-4-data-analysis.ipynb)\n",
    "*  Notebook 5: Class Project \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5ec74-c1d5-457e-93be-3ca19d8a5c23",
   "metadata": {},
   "source": [
    "# **Section 1**: Twitter Dataset\n",
    "Here is the research paper\\\n",
    "[Post-January 6th deplatforming reduced the reach of misinformation on Twitter](https://www.nature.com/articles/s41586-024-07524-8)\n",
    "\n",
    "The dataset that accompanies this paper has been compiled and included below as a Pandas dataframe (assigned to the variable `mccabe_data`). Please base your main analyses on this shared dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3225f8-a131-4ccc-9de8-869b86db17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary packages for initial analysis\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e51077ae-9c0a-4141-90c0-d3d65cad5f87",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/_old_home/compss-214a/mccabe-public-data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Importing dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m mccabe_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m~/_old_home/compss-214a/mccabe-public-data.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/jovyan/_old_home/compss-214a/mccabe-public-data.csv'"
     ]
    }
   ],
   "source": [
    "#Importing dataset\n",
    "mccabe_data = pd.read_csv('/home/jovyan/compss-214a/mccabe-public-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e8541c-432c-4bda-94f2-b621308a90a5",
   "metadata": {},
   "source": [
    "You are welcome to rename the dataset or work with different subsets of this data or with additional datasets if neccesary, but this shared dataset should be the primary source for your analyses, so that we are all working with the same underlying source of information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b5be5c-b799-4e53-8d17-32444ad8671f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Section 2** Exploring the structure of the dataset\n",
    "Describe the key variables you are interested in. Feel free to include data summaries and/or vizualizations that illustrate how the dataset is structured, such as the different groups of users you are interested in and the different measures of whether posts are classified as misinformation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d19efc04-4740-436f-b893-3b9d8bba647f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>fake_merged</th>\n",
       "      <th>fake_merged_initiation</th>\n",
       "      <th>fake_merged_rt</th>\n",
       "      <th>fake_grinberg_initiation</th>\n",
       "      <th>fake_grinberg_rt</th>\n",
       "      <th>fake_grinberg_rb_initiation</th>\n",
       "      <th>fake_grinberg_rb_rt</th>\n",
       "      <th>fake_newsguard_initiation</th>\n",
       "      <th>fake_newsguard_rt</th>\n",
       "      <th>...</th>\n",
       "      <th>not_fake_shopping</th>\n",
       "      <th>not_fake_shopping_initiation</th>\n",
       "      <th>not_fake_shopping_rt</th>\n",
       "      <th>not_fake_sports</th>\n",
       "      <th>not_fake_sports_initiation</th>\n",
       "      <th>not_fake_sports_rt</th>\n",
       "      <th>n</th>\n",
       "      <th>stat</th>\n",
       "      <th>nusers</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>875.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>653.0</td>\n",
       "      <td>...</td>\n",
       "      <td>196.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12387.0</td>\n",
       "      <td>total</td>\n",
       "      <td>4390</td>\n",
       "      <td>fns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>3382.0</td>\n",
       "      <td>825.0</td>\n",
       "      <td>2557.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>941.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>2293.0</td>\n",
       "      <td>...</td>\n",
       "      <td>608.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>54897.0</td>\n",
       "      <td>total</td>\n",
       "      <td>11629</td>\n",
       "      <td>fns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-12-02</td>\n",
       "      <td>3644.0</td>\n",
       "      <td>992.0</td>\n",
       "      <td>2652.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>2455.0</td>\n",
       "      <td>...</td>\n",
       "      <td>684.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>68505.0</td>\n",
       "      <td>total</td>\n",
       "      <td>13132</td>\n",
       "      <td>fns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>1110.0</td>\n",
       "      <td>3074.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>921.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>562.0</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>2890.0</td>\n",
       "      <td>...</td>\n",
       "      <td>782.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>74502.0</td>\n",
       "      <td>total</td>\n",
       "      <td>13997</td>\n",
       "      <td>fns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-12-04</td>\n",
       "      <td>4436.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>3336.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>3146.0</td>\n",
       "      <td>...</td>\n",
       "      <td>540.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>71762.0</td>\n",
       "      <td>total</td>\n",
       "      <td>13967</td>\n",
       "      <td>fns</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  fake_merged  fake_merged_initiation  fake_merged_rt  \\\n",
       "0  2019-11-30        875.0                   199.0           676.0   \n",
       "1  2019-12-01       3382.0                   825.0          2557.0   \n",
       "2  2019-12-02       3644.0                   992.0          2652.0   \n",
       "3  2019-12-03       4184.0                  1110.0          3074.0   \n",
       "4  2019-12-04       4436.0                  1100.0          3336.0   \n",
       "\n",
       "   fake_grinberg_initiation  fake_grinberg_rt  fake_grinberg_rb_initiation  \\\n",
       "0                      74.0             207.0                         42.0   \n",
       "1                     257.0             941.0                        120.0   \n",
       "2                     280.0             780.0                        141.0   \n",
       "3                     339.0             921.0                        185.0   \n",
       "4                     307.0            1171.0                        135.0   \n",
       "\n",
       "   fake_grinberg_rb_rt  fake_newsguard_initiation  fake_newsguard_rt  ...  \\\n",
       "0                138.0                      188.0              653.0  ...   \n",
       "1                546.0                      760.0             2293.0  ...   \n",
       "2                479.0                      926.0             2455.0  ...   \n",
       "3                562.0                     1052.0             2890.0  ...   \n",
       "4                540.0                     1038.0             3146.0  ...   \n",
       "\n",
       "   not_fake_shopping  not_fake_shopping_initiation  not_fake_shopping_rt  \\\n",
       "0              196.0                          61.0                 135.0   \n",
       "1              608.0                         207.0                 401.0   \n",
       "2              684.0                         289.0                 395.0   \n",
       "3              782.0                         236.0                 546.0   \n",
       "4              540.0                         261.0                 279.0   \n",
       "\n",
       "   not_fake_sports  not_fake_sports_initiation  not_fake_sports_rt        n  \\\n",
       "0             16.0                         7.0                 9.0  12387.0   \n",
       "1             99.0                        33.0                66.0  54897.0   \n",
       "2             82.0                        37.0                45.0  68505.0   \n",
       "3             92.0                        41.0                51.0  74502.0   \n",
       "4            124.0                        53.0                71.0  71762.0   \n",
       "\n",
       "    stat  nusers  group  \n",
       "0  total    4390    fns  \n",
       "1  total   11629    fns  \n",
       "2  total   13132    fns  \n",
       "3  total   13997    fns  \n",
       "4  total   13967    fns  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calling head to assess the columns and values of this dataset\n",
    "mccabe_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "573b8557-b535-4b86-8458-2433df9fc93f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                                2019-11-30\n",
       "fake_merged                              875.0\n",
       "fake_merged_initiation                   199.0\n",
       "fake_merged_rt                           676.0\n",
       "fake_grinberg_initiation                  74.0\n",
       "fake_grinberg_rt                         207.0\n",
       "fake_grinberg_rb_initiation               42.0\n",
       "fake_grinberg_rb_rt                      138.0\n",
       "fake_newsguard_initiation                188.0\n",
       "fake_newsguard_rt                        653.0\n",
       "not_fake                               11512.0\n",
       "not_fake_initiation                     4357.0\n",
       "not_fake_rt                             7155.0\n",
       "not_fake_conservative                    529.0\n",
       "not_fake_conservative_initiation         156.0\n",
       "not_fake_conservative_rt                 373.0\n",
       "not_fake_liberal                        1030.0\n",
       "not_fake_liberal_initiation              247.0\n",
       "not_fake_liberal_rt                      783.0\n",
       "not_fake_shopping                        196.0\n",
       "not_fake_shopping_initiation              61.0\n",
       "not_fake_shopping_rt                     135.0\n",
       "not_fake_sports                           16.0\n",
       "not_fake_sports_initiation                 7.0\n",
       "not_fake_sports_rt                         9.0\n",
       "n                                      12387.0\n",
       "stat                                     total\n",
       "nusers                                    4390\n",
       "group                                      fns\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calling iloc to see a full list of the different misinformation sharing groups\n",
    "mccabe_data.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0c10f-aa8c-4279-be96-e3e64bd3d272",
   "metadata": {},
   "source": [
    "The variables I observed that appear the most interesting for analysis are the 'fns' and 'nfns' groups, which account for the misinformation and non-misinformation sharers respectively. These two groups, in my opinion, will be our prime candidates for identifying the most significant distinction between the before and after of the Twitter suspension. I will also have to do a lot of manipulation of the date column, due to the fact that we will be using DiD so identifying the pre and post treatment categories of the dataset will be dependant upon this. The 'fake_merged' variable will also be important to analyse, due to the fact that it will be giving us the total number of tweets that are identified as containing fake news.\n",
    "\n",
    "For my initial looks at the dataset, I used '.head()' to look at the top of the data and see the different columns and values and 'iloc' to view the different types of misinformation tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d2784-8f7c-4f34-a556-3758f1612fe0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Section 3** Replication of Main DiD Results\n",
    "In this section, you will perform at least one Difference in Differences analysis with the goal of conceptually replicating the key DiD analysis that McCabe et al performed to support their primary conclusion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b5b696d5-0246-491e-9bd8-14ccf9ee447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting data column to timedate\n",
    "mccabe_data['date'] = pd.to_datetime(mccabe_data['date'])\n",
    "\n",
    "#Setting the suspension date in dataset\n",
    "# Specify a datetime object for the start of the suspension period\n",
    "suspension_start = pd.to_datetime('2021-01-08')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0c549928-8944-4a55-9199-7b7ac3aec003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83/2665920851.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  total_mccabe_df['post_treatment'] = (total_mccabe_df['date'] > deplatforming_date).astype(int)\n",
      "/tmp/ipykernel_83/2665920851.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  total_mccabe_df['treatment_group'] = (total_mccabe_df['group'] == 'fns').astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>fake_merged</th>\n",
       "      <th>fake_merged_initiation</th>\n",
       "      <th>fake_merged_rt</th>\n",
       "      <th>fake_grinberg_initiation</th>\n",
       "      <th>fake_grinberg_rt</th>\n",
       "      <th>fake_grinberg_rb_initiation</th>\n",
       "      <th>fake_grinberg_rb_rt</th>\n",
       "      <th>fake_newsguard_initiation</th>\n",
       "      <th>fake_newsguard_rt</th>\n",
       "      <th>...</th>\n",
       "      <th>not_fake_shopping_rt</th>\n",
       "      <th>not_fake_sports</th>\n",
       "      <th>not_fake_sports_initiation</th>\n",
       "      <th>not_fake_sports_rt</th>\n",
       "      <th>n</th>\n",
       "      <th>stat</th>\n",
       "      <th>nusers</th>\n",
       "      <th>group</th>\n",
       "      <th>post_treatment</th>\n",
       "      <th>treatment_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22232</th>\n",
       "      <td>2020-06-30</td>\n",
       "      <td>54.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>141.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21516.0</td>\n",
       "      <td>total</td>\n",
       "      <td>13458</td>\n",
       "      <td>nfns</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22305</th>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>160.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1273.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>115505.0</td>\n",
       "      <td>total</td>\n",
       "      <td>55585</td>\n",
       "      <td>nfns</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22357</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>65.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>105369.0</td>\n",
       "      <td>total</td>\n",
       "      <td>50210</td>\n",
       "      <td>nfns</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-12-18</td>\n",
       "      <td>5773.0</td>\n",
       "      <td>1211.0</td>\n",
       "      <td>4562.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>1146.0</td>\n",
       "      <td>4310.0</td>\n",
       "      <td>...</td>\n",
       "      <td>546.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>83465.0</td>\n",
       "      <td>total</td>\n",
       "      <td>14775</td>\n",
       "      <td>fns</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22512</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>201.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>95183.0</td>\n",
       "      <td>total</td>\n",
       "      <td>46056</td>\n",
       "      <td>nfns</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6074</th>\n",
       "      <td>2020-02-20</td>\n",
       "      <td>587.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>115447.0</td>\n",
       "      <td>total</td>\n",
       "      <td>54373</td>\n",
       "      <td>nfns</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2020-06-04</td>\n",
       "      <td>7302.0</td>\n",
       "      <td>1675.0</td>\n",
       "      <td>5627.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>1812.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>995.0</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>5159.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>163749.0</td>\n",
       "      <td>total</td>\n",
       "      <td>18879</td>\n",
       "      <td>fns</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6027</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>653.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>...</td>\n",
       "      <td>796.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>91468.0</td>\n",
       "      <td>total</td>\n",
       "      <td>43248</td>\n",
       "      <td>nfns</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-12-07</td>\n",
       "      <td>3667.0</td>\n",
       "      <td>865.0</td>\n",
       "      <td>2802.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>821.0</td>\n",
       "      <td>2695.0</td>\n",
       "      <td>...</td>\n",
       "      <td>402.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>57632.0</td>\n",
       "      <td>total</td>\n",
       "      <td>12227</td>\n",
       "      <td>fns</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>2020-11-11</td>\n",
       "      <td>16038.0</td>\n",
       "      <td>2976.0</td>\n",
       "      <td>13062.0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>4344.0</td>\n",
       "      <td>658.0</td>\n",
       "      <td>3113.0</td>\n",
       "      <td>2848.0</td>\n",
       "      <td>12776.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1226.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>156215.0</td>\n",
       "      <td>total</td>\n",
       "      <td>19645</td>\n",
       "      <td>fns</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  fake_merged  fake_merged_initiation  fake_merged_rt  \\\n",
       "22232 2020-06-30         54.0                    14.0            40.0   \n",
       "22305 2020-09-11        160.0                    63.0            97.0   \n",
       "22357 2020-11-02         65.0                    16.0            49.0   \n",
       "18    2019-12-18       5773.0                  1211.0          4562.0   \n",
       "22512 2021-04-06        201.0                    56.0           145.0   \n",
       "6074  2020-02-20        587.0                   117.0           470.0   \n",
       "187   2020-06-04       7302.0                  1675.0          5627.0   \n",
       "6027  2020-01-04        653.0                   338.0           315.0   \n",
       "7     2019-12-07       3667.0                   865.0          2802.0   \n",
       "12974 2020-11-11      16038.0                  2976.0         13062.0   \n",
       "\n",
       "       fake_grinberg_initiation  fake_grinberg_rt  \\\n",
       "22232                      12.0              12.0   \n",
       "22305                      20.0              41.0   \n",
       "22357                       5.0              20.0   \n",
       "18                        396.0            1044.0   \n",
       "22512                      17.0              32.0   \n",
       "6074                       42.0             146.0   \n",
       "187                       527.0            1812.0   \n",
       "6027                       72.0              82.0   \n",
       "7                         253.0             891.0   \n",
       "12974                     939.0            4344.0   \n",
       "\n",
       "       fake_grinberg_rb_initiation  fake_grinberg_rb_rt  \\\n",
       "22232                          7.0                  6.0   \n",
       "22305                         19.0                 20.0   \n",
       "22357                          3.0                 11.0   \n",
       "18                           251.0                579.0   \n",
       "22512                          8.0                  5.0   \n",
       "6074                          34.0                101.0   \n",
       "187                          243.0                995.0   \n",
       "6027                          39.0                 55.0   \n",
       "7                            118.0                503.0   \n",
       "12974                        658.0               3113.0   \n",
       "\n",
       "       fake_newsguard_initiation  fake_newsguard_rt  ...  \\\n",
       "22232                       14.0               40.0  ...   \n",
       "22305                       63.0               82.0  ...   \n",
       "22357                       16.0               45.0  ...   \n",
       "18                        1146.0             4310.0  ...   \n",
       "22512                       56.0              130.0  ...   \n",
       "6074                       111.0              446.0  ...   \n",
       "187                       1589.0             5159.0  ...   \n",
       "6027                       319.0              306.0  ...   \n",
       "7                          821.0             2695.0  ...   \n",
       "12974                     2848.0            12776.0  ...   \n",
       "\n",
       "       not_fake_shopping_rt  not_fake_sports  not_fake_sports_initiation  \\\n",
       "22232                 141.0             33.0                        13.0   \n",
       "22305                1273.0            394.0                       100.0   \n",
       "22357                1447.0            160.0                        65.0   \n",
       "18                    546.0             71.0                        25.0   \n",
       "22512                1126.0            172.0                        82.0   \n",
       "6074                 1158.0            379.0                       110.0   \n",
       "187                  1001.0            160.0                        65.0   \n",
       "6027                  796.0            300.0                        82.0   \n",
       "7                     402.0            121.0                        59.0   \n",
       "12974                1226.0            131.0                        27.0   \n",
       "\n",
       "       not_fake_sports_rt         n   stat  nusers  group  post_treatment  \\\n",
       "22232                20.0   21516.0  total   13458   nfns               0   \n",
       "22305               294.0  115505.0  total   55585   nfns               0   \n",
       "22357                95.0  105369.0  total   50210   nfns               0   \n",
       "18                   46.0   83465.0  total   14775    fns               0   \n",
       "22512                90.0   95183.0  total   46056   nfns               1   \n",
       "6074                269.0  115447.0  total   54373   nfns               0   \n",
       "187                  95.0  163749.0  total   18879    fns               0   \n",
       "6027                218.0   91468.0  total   43248   nfns               0   \n",
       "7                    62.0   57632.0  total   12227    fns               0   \n",
       "12974               104.0  156215.0  total   19645    fns               0   \n",
       "\n",
       "       treatment_group  \n",
       "22232                0  \n",
       "22305                0  \n",
       "22357                0  \n",
       "18                   1  \n",
       "22512                0  \n",
       "6074                 0  \n",
       "187                  1  \n",
       "6027                 0  \n",
       "7                    1  \n",
       "12974                1  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering dataframe to only include total numbers of tweets\n",
    "total_df = mccabe_data[(mccabe_data['stat'] == 'total')]\n",
    "\n",
    "# Filter the DataFrame to include only identified misinformation sharers ('fns') and non-misinformation sharers ('nfns').\n",
    "total_mccabe_df = total_df[total_df['group'].isin(['fns' , 'nfns'])]\n",
    "\n",
    "# Define the deplatforming date (January 8, 2021), which will be used as the comparison point for pre and post treatment.\n",
    "deplatforming_date = pd.to_datetime('2021-01-08')\n",
    "\n",
    "# Create a 'post_treatment' variable to indicate whether the data point is after the deplatforming event.\n",
    "# If the 'date' is greater than the deplatforming date, it is labeled as post-treatment (1)and pre-treatment (0) if lower.\n",
    "total_mccabe_df['post_treatment'] = (total_mccabe_df['date'] > deplatforming_date).astype(int)\n",
    "\n",
    "# Create a 'treatment_group' variable to distinguish the treatment group (misinformation sharers, 'fns') from the control group (non-misinformation sharers, 'nfns').\n",
    "total_mccabe_df['treatment_group'] = (total_mccabe_df['group'] == 'fns').astype(int)\n",
    "total_mccabe_df = total_mccabe_df.sort_values(by = 'date')\n",
    "total_mccabe_df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d0837d0-fe5b-4501-9f21-6cd46e0a799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First object in your fomula is your outcome/ dependent variable, next two are your two predictors and the last variable is your interaction which is predictor 1* predictor 2\n",
    "formula = 'fake_merged ~ post_suspension + treatment_group + post_suspension*treatment_group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6469d78-6a33-44b4-ad69-0acf5a962cd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "PatsyError",
     "evalue": "Error evaluating factor: NameError: name 'post_suspension' is not defined\n    fake_merged ~ post_suspension + treatment_group + post_suspension*treatment_group\n                  ^^^^^^^^^^^^^^^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/lib/python3.11/site-packages/patsy/compat.py:36\u001b[0m, in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/srv/conda/lib/python3.11/site-packages/patsy/eval.py:169\u001b[0m, in \u001b[0;36mEvalEnvironment.eval\u001b[0;34m(self, expr, source_name, inner_namespace)\u001b[0m\n\u001b[1;32m    168\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcompile\u001b[39m(expr, source_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28meval\u001b[39m(code, {}, VarLookupDict([inner_namespace]\n\u001b[1;32m    170\u001b[0m                                     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_namespaces))\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'post_suspension' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPatsyError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Creating and fitting your model, using the total mccabe dataframe, and printing results for analysis.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_mccabe_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(results\u001b[38;5;241m.\u001b[39msummary())\n",
      "File \u001b[0;32m/srv/conda/lib/python3.11/site-packages/statsmodels/base/model.py:200\u001b[0m, in \u001b[0;36mModel.from_formula\u001b[0;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# with patsy it's drop or raise. let's raise.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 200\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_formula_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m ((endog, exog), missing_idx, design_info) \u001b[38;5;241m=\u001b[39m tmp\n\u001b[1;32m    203\u001b[0m max_endog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_formula_max_endog\n",
      "File \u001b[0;32m/srv/conda/lib/python3.11/site-packages/statsmodels/formula/formulatools.py:63\u001b[0m, in \u001b[0;36mhandle_formula_data\u001b[0;34m(Y, X, formula, depth, missing)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_util\u001b[38;5;241m.\u001b[39m_is_using_pandas(Y, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 63\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mdmatrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataframe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m         result \u001b[38;5;241m=\u001b[39m dmatrices(formula, Y, depth, return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataframe\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     67\u001b[0m                            NA_action\u001b[38;5;241m=\u001b[39mna_action)\n",
      "File \u001b[0;32m/srv/conda/lib/python3.11/site-packages/patsy/highlevel.py:309\u001b[0m, in \u001b[0;36mdmatrices\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct two design matrices given a formula_like and data.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03mThis function is identical to :func:`dmatrix`, except that it requires\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03mSee :func:`dmatrix` for details.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    308\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m EvalEnvironment\u001b[38;5;241m.\u001b[39mcapture(eval_env, reference\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 309\u001b[0m (lhs, rhs) \u001b[38;5;241m=\u001b[39m \u001b[43m_do_highlevel_design\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformula_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lhs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PatsyError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel is missing required outcome variables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/srv/conda/lib/python3.11/site-packages/patsy/highlevel.py:164\u001b[0m, in \u001b[0;36m_do_highlevel_design\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_iter_maker\u001b[39m():\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([data])\n\u001b[0;32m--> 164\u001b[0m design_infos \u001b[38;5;241m=\u001b[39m \u001b[43m_try_incr_builders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformula_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m design_infos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m build_design_matrices(design_infos, data,\n\u001b[1;32m    168\u001b[0m                                  NA_action\u001b[38;5;241m=\u001b[39mNA_action,\n\u001b[1;32m    169\u001b[0m                                  return_type\u001b[38;5;241m=\u001b[39mreturn_type)\n",
      "File \u001b[0;32m/srv/conda/lib/python3.11/site-packages/patsy/highlevel.py:66\u001b[0m, in \u001b[0;36m_try_incr_builders\u001b[0;34m(formula_like, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formula_like, ModelDesc):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_env, EvalEnvironment)\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdesign_matrix_builders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mformula_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlhs_termlist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mformula_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrhs_termlist\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/lib/python3.11/site-packages/patsy/build.py:693\u001b[0m, in \u001b[0;36mdesign_matrix_builders\u001b[0;34m(termlists, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m    689\u001b[0m factor_states \u001b[38;5;241m=\u001b[39m _factors_memorize(all_factors, data_iter_maker, eval_env)\n\u001b[1;32m    690\u001b[0m \u001b[38;5;66;03m# Now all the factors have working eval methods, so we can evaluate them\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;66;03m# on some data to find out what type of data they return.\u001b[39;00m\n\u001b[1;32m    692\u001b[0m (num_column_counts,\n\u001b[0;32m--> 693\u001b[0m  cat_levels_contrasts) \u001b[38;5;241m=\u001b[39m \u001b[43m_examine_factor_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_factors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfactor_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;66;03m# Now we need the factor infos, which encapsulate the knowledge of\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;66;03m# how to turn any given factor into a chunk of data:\u001b[39;00m\n\u001b[1;32m    699\u001b[0m factor_infos \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/srv/conda/lib/python3.11/site-packages/patsy/build.py:443\u001b[0m, in \u001b[0;36m_examine_factor_types\u001b[0;34m(factors, factor_states, data_iter_maker, NA_action)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_iter_maker():\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(examine_needed):\n\u001b[0;32m--> 443\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mfactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactor_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m factor \u001b[38;5;129;01min\u001b[39;00m cat_sniffers \u001b[38;5;129;01mor\u001b[39;00m guess_categorical(value):\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m factor \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cat_sniffers:\n",
      "File \u001b[0;32m/srv/conda/lib/python3.11/site-packages/patsy/eval.py:568\u001b[0m, in \u001b[0;36mEvalFactor.eval\u001b[0;34m(self, memorize_state, data)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m, memorize_state, data):\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/lib/python3.11/site-packages/patsy/eval.py:551\u001b[0m, in \u001b[0;36mEvalFactor._eval\u001b[0;34m(self, code, memorize_state, data)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_eval\u001b[39m(\u001b[38;5;28mself\u001b[39m, code, memorize_state, data):\n\u001b[1;32m    550\u001b[0m     inner_namespace \u001b[38;5;241m=\u001b[39m VarLookupDict([data, memorize_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransforms\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_and_wrap_exc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError evaluating factor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m                             \u001b[49m\u001b[43minner_namespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_namespace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/lib/python3.11/site-packages/patsy/compat.py:43\u001b[0m, in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     new_exc \u001b[38;5;241m=\u001b[39m PatsyError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m                          \u001b[38;5;241m%\u001b[39m (msg, e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e),\n\u001b[1;32m     41\u001b[0m                          origin)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Use 'exec' to hide this syntax from the Python 2 parser:\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     exec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise new_exc from e\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# In python 2, we just let the original exception escape -- better\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# than destroying the traceback. But if it's a PatsyError, we can\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# at least set the origin properly.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, PatsyError):\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mPatsyError\u001b[0m: Error evaluating factor: NameError: name 'post_suspension' is not defined\n    fake_merged ~ post_suspension + treatment_group + post_suspension*treatment_group\n                  ^^^^^^^^^^^^^^^"
     ]
    }
   ],
   "source": [
    "#Creating and fitting your model, using the total mccabe dataframe, and printing results for analysis.\n",
    "model = smf.ols(formula, data=total_mccabe_df)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48ef50-486f-4b60-bf8c-28e167163922",
   "metadata": {},
   "source": [
    "After running my Difference in Differences analysis and producing my regression table, I can see the variation and interaction of my treatment groups. From our table I can notice that the intercept is 368, which represents the number of fake tweets identified, without taking into account the deplatforming or the effects of the deplatforming afterwards. The regression analysis examines the impact of being in the treatment group and the timing of a suspension period (pre vs. post) on the dependent variable fake_merged. The model has an R-squared value of 0.833, meaning that 83.3% of the variation in the dependent variable is explained by the independent variables in the model. This indicates a strong fit between the model and the data. The analysis suggests that being in the treatment group has a large positive impact on the dependent variable, but this effect is significantly diminished after the suspension period. The suspension period alone does not have a statistically significant impact on the control group. However, for the treatment group, the post-suspension interaction effect is strong and negative, suggesting a substantial decline in the number of fake tweets made following the suspension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633de82f-54d0-490c-8b34-3dfa66cd70e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting results to visualize effects reflected in DiD model.\n",
    "plt.figure(figsize=(16, 2))\n",
    "\n",
    "# plot post counts over time for accounts that were eventually Deplatformed\n",
    "plt.plot(total_mccabe_df.date, total_mccabe_df.fake_merged)\n",
    "\n",
    "# Specify a datetime object for the start of the suspension period\n",
    "suspension_start = pd.to_datetime('2021-01-06')\n",
    "\n",
    "# Specify a datetime object for the end of the suspension period\n",
    "suspension_end = pd.to_datetime('2021-01-12')\n",
    "\n",
    "# Plot vertical lines at the start and end of the suspension period\n",
    "plt.axvline(suspension_start, color='r', linestyle='--', label='Suspension Starts (January 6th 2021)')\n",
    "plt.axvline(suspension_end, color='g', linestyle='--', label='Suspension Ends (January 12th 2021)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78299bd7-2e9f-445d-b548-0654e2234306",
   "metadata": {},
   "source": [
    "Looking at our line plot we can also visually observe the decrease in fake news tweeted out following the suspension period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97aded-28cd-4e7e-9d56-389c145577cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Section 4** Extensions and follow up analyses\n",
    "In this section, you will perform follow-up analyses, summaries, or visualizations that you feel help shed light on the robustness of the conclusion reached by McCabe et al. You are welcome to draw on insights you gained through data simulation, and to draw on the questions we discussed in class surrounding the **key assumptions and study decisions** in [Notebook 1: Data Acquisition](https://github.com/ccs-ucb/CSS1A/blob/main/notebooks/notebook-1-data-acquisition.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ae86c-7def-4809-8496-3cc8244aedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages necessary for PCA analysis\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3af8e-22d7-44b4-84b6-f6f28abe8acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out dataframe to only include totals once again.\n",
    "df = mccabe_data[mccabe_data['stat'] == 'total'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1532f3-8cc3-4a62-ae31-f0dfd9717a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up X and Y variables to determing predictors and outcomes.\n",
    "X = df.drop(columns = ['stat', 'group', 'date'])\n",
    "y= df['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d74b6-350d-4444-a11d-cbc33dd25b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining our scaler.\n",
    "scaled = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c993194d-3492-48a9-ab92-6325613fa984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling our data and fitting it to our X predictors.\n",
    "X_scaled = scaled.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c4296-e761-4dc5-b4cf-a8e770c62e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating our PCA variable and setting 3 components as a safe number to account for a majority of our variation.\n",
    "pca = PCA(n_components = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac080983-cb88-4a21-b84f-7f8b68bf707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting our PCA object to our scaled X predictors.\n",
    "pca.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d7c5b-8ab2-41a9-b96f-bd73d46b09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "total_explained_variance_ratio = explained_variance_ratio.sum()\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nExplained Variance Ratio:\\n{explained_variance_ratio}\")\n",
    "print(f\"Total Explained Variance Ratio: {total_explained_variance_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178eaa6-30e7-483c-b50c-24a2aa35b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries to create Scree plot to visualize PCA.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "plt.plot(cumulative_variance_ratio, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio by Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7ad5c-fa8e-4a65-bc3c-f892069e59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing resulting PCA dataframe to see which categories of tweets cover the most variation and contribute greated to the determination of user group\n",
    "pca_df = pd.DataFrame(pca.components_, columns = X.columns)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4c55d-cbfe-4bc3-8563-0f53a08fea2f",
   "metadata": {},
   "source": [
    "I decided to use Principal Component Analysis (PCA) to attempt to reduce the dimensionality of the data and identify the key variables contributing to the variation in misinformation sharing. After scaling the dataset, PCA was applied with three principal components, which accounted for approximately 92.6% of the total variance. This indicates that a majority of the variation in the data could be captured by just these three components, simplifying the complexity of the dataset.\n",
    "\n",
    "The PCA loadings revealed the variables most associated with misinformation sharing, such as fake_merged, fake_merged_initiation, and fake_merged_rt. These variables had high contributions to the first principal component, showing their importance in distinguishing between misinformation sharers and non-sharers. A scree plot was also used to visualize the cumulative explained variance by the principal components, confirming that most of the variance could be explained by the top three components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a2893-84fe-4cb5-84b0-0e8c691537a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Section 5** Conclusions and Reflections\n",
    "Here is where you draw together insights you have gained by analyzing this dataset and reflections on the methods we have applied. You should provide a clear answer to the question: \n",
    "\n",
    "> What are your conclusions about the question posed in this assignment: **Did deplatforming reduce misinformation on Twitter?**\n",
    "\n",
    "You are welcome to use the bullet points below to guide your reflections if they are helpful, and also to include any additional insights. \n",
    "\n",
    "*  Is the current dataset sufficient to offer insight into this question? What are some key limitations of the dataset, and key merits?\n",
    "*  Is the DiD method sufficient to support strong conclusions related to this question?\n",
    "*  Overall, do you think the conclusions of McCabe et al. (2024) are justified?\n",
    "*  More generally, do you feel that misinformation on social media is a substantial threat to discourse and society that data science can address, and how has this project influenced your view?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88562355-e4e9-4e3a-8605-948313a21a61",
   "metadata": {},
   "source": [
    "In this analysis of the McCabe dataset, the goal was to determine whether deplatforming efforts, particularly following the January 6th Capitol riots, reduced the spread of misinformation on Twitter. Using a Difference-in-Differences (DiD) approach, the study compared misinformation sharing between a treatment group (those who shared misinformation, labeled as \"fns\") and a control group (non-misinformation sharers, labeled as \"nfns\"), before and after the deplatforming event on January 8th, 2021. The regression results show that the deplatforming had a significant effect on misinformation spread. The interaction term between the treatment group and the post-suspension period was negative and highly significant (coefficient = -4355.65, p < 0.001), suggesting that the number of fake tweets posted by the treatment group decreased sharply after deplatforming. This supports the hypothesis that the removal of key misinformation accounts significantly reduced the spread of misinformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d07a8-40aa-42b9-b8f9-66fd7597b867",
   "metadata": {},
   "source": [
    "The coefficient for the treatment_group alone was positive and large (7603.62), indicating that misinformation sharers produced significantly more fake tweets compared to non-misinformation sharers before the suspension. However, the negative interaction term suggests that the deplatforming mitigated this effect post-suspension, leading to a substantial reduction in misinformation for the treatment group. Interestingly, the post-suspension period on its own (without interaction) did not significantly affect the control group (non-misinformation sharers). This suggests that the deplatforming specifically targeted and effectively reduced misinformation from the treatment group, rather than influencing the general behavior of users on Twitter. The model had a high R-squared value of 0.833, meaning that 83.3% of the variation in misinformation (measured by fake_merged) was explained by the model's variables. This strong fit supports the reliability of the conclusions drawn from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6fb8e-7b55-4a19-995d-1f950474ed2a",
   "metadata": {},
   "source": [
    "While the dataset captures detailed information on misinformation sharing behavior, it may not fully account for external factors (e.g., changes in platform policies or user behavior) that could also influence the spread of misinformation. The dataset is focused on Twitter, so the conclusions cannot be generalized to other social media platforms. While the analysis confirms that deplatforming reduced misinformation on Twitter, it does not assess whether these users migrated to other platforms to continue sharing misinformation, which could limit the broader effectiveness of such interventions. Future research could explore the migration of deplatformed users to alternative social media platforms and the overall effectiveness of deplatforming as a strategy for combating misinformation across the digital ecosystem. I do feel that this analysis has given me more context as to the effects of social media on the spread of misinformation, however it has not changed my views. I still feel that social media is, at the end of the day, a tool that can be used for good or bad reasons. When assessing the effects of such a tool, it must always be acknowledged who is using that too and for what purpose without ascribing good or bad labels unto the tool itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d738c-f11a-4936-a9a6-09c2cb9c27de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Section 6** Use of External Resources\n",
    "Please indicate here your use of external resources such as coding assitants or other AI systems to aid in completing this assignment. Please select one of the options below by placing an `[x]` next to the relevant option. You may also include any additional notes that may help gradeers assess your reliance on external resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022eaad6-6edd-4591-960f-a52dbf1ba999",
   "metadata": {},
   "source": [
    "## No Usage\n",
    "`[]` I attest that I did not use a coding assistant such as ChatGPT or other large language models to complete this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b0f53-1b5f-4329-b2b5-25ac24d262d4",
   "metadata": {},
   "source": [
    "## Declared Usage\n",
    "`[x]` I made use of a coding assistant such as ChatGPT or other large language models to complete this assignment.\n",
    "\n",
    "If you select this option, you are required to include a record of your interaction with the coding assistant here. Please include in the cell below either a link to the transcript or the transcript itself. If you provide a link, it is your responsibility to ensure that the link works and can be accessed by the graders. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847fb332-2e59-4b8b-a109-3e14244c05da",
   "metadata": {},
   "source": [
    "### Transcript\n",
    "https://chatgpt.com/share/6713252f-472c-8007-bf55-02a8dce24906"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759959c7-727c-4a3a-adbc-4a024937110e",
   "metadata": {},
   "source": [
    "### Additional Notes\n",
    "**Any additional notes on your use of a codign assistant go here.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
